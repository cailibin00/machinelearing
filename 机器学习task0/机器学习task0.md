# Task0:机器学习入门
***
经过焦糖招新网站提供的学习资料，在将近一周的学习之后，我认为基本能够掌握task0之中所提到的概念，因此记下该学习笔记

### 监督学习与无监督学习的区别
先来写一下对两者的认识：结合专业的定义和我自己的理解

* **监督学习**：
1. 概念：监督学习是一种机器学习方法，其中模型从**标记**的训练数据学习，以便能够预测未见过的数据的输出。这些数据集包括特征和标签。监督学习的目标是学习一个**映射规则**，将输入特征映射到输出标签。
2. 应用：分类（如task2之中图像识别，垃圾邮箱识别）、回归（如task1之中的房价预测、股票价格预测）。
3. 常见模型：CNN模型，MLP模型等
4. 评估的标准：损失值，准确率等

* **无监督学习**：
1. 概念：无监督学习涉及从**未标记**的数据中发现模式或结构，为了计算出数据的**内在结构和关系**，而不是预测特定输出。
2. 评估的标准：依赖于特定的评估标准

* **两者差别**
1. 训练数据标记不同：监督学习从标记的训练数据学习，无监督学习的训练数据则不存在标记
2. 评估标准：根据上文提到的评估标准，监督学习的评价标准比较固定，而无监督学习的评价标准比较主观
3. 当然，两者的利用模型肯定是不同的


### 机器学习与深度学习的区别
在上文我们提到，监督学习和无监督学习，但是这两者都是属于机器学习和深度学习，下面我们就来了解机器学习和深度学习
![图片](/模型.png)
* **机器学习**：
1. 概念：机器学习是人工智能的一部分，它涵盖了开发计算机程序以从数据中学习和做出决策或预测的方法。机器学习模型可以通过各种算法实现
2. 方法：包括我们上面所说的监督学习和无监督学习，还有强化学习等

* **深度学习**：
1. 概念：它也是人工智能的一部分，还是机器学习的一个一部分，使用神经网络结构（包含深度神经网络）来学习，深度神经网络结构中，具有多个隐藏层
2. 方法：主要是监督学习和无监督学习

* **差别**
1. 训练数据大小：通常深度学习需要较大量的数据来训练模型
2. 可解释性：深度学习由于我们提到的有多个隐藏层，所以可解释性较差，
3. 特征工程：机器学习通常需要特征工程，使得预测更准确；深度学习能够自动从原始数据学习特征，减少手动操作
4. 深度学习是机器学习的一部分，机器学习包含更广阔的算法技术
<img scr="/概念包含图.jpg" width="60%">

### 深度学习的数学
当然，人工智能肯定离不开大量的数据计算，无论是机器学习还是深度学习，都离不开数字的加减乘除，那么人工智能靠什么算法实现大量数据的快速计算，又有那些数学原理构建起了深度学习的大厦？让我们娓娓道来

* **偏导数**
在机器学习中，偏导数用于优化模型：在训练模型时，我们求损失函数来评估模型的好坏，通过计算损失函数对每个参数的偏导数，权重的偏导数相乘得到梯度，通过梯度的方向，就可以知道参数朝着哪个方向移动，模型的损失值能够达到最小。所以说，通过偏导数，我们可以实现调整参数，得到更好的模型

* **链式法则**
链式法则告诉我们如何计算由多个函数**复合**而成的函数的导数。在机器学习中，链式法则在神经网络反向传播的时候，用于从输出层向输入层反向传播误差，计算每个权重对最终误差的梯度
![图片](/链式法则.webp)

### 梯度
由于上文提到梯度，因此略讲。梯度指向函数增长最快的方向。在优化参数的时候中，我们加上负号，乘以学习率，使得梯度的反方向移动，称为**梯度下降**，以找到损失函数的最小值，用来减小误差,我们使用一个3D图来可视化，黑色箭头的方向就是梯度下降的方向，以达到最小的损失值
![图片](/梯度.jpg)

### 矩阵和线性代数
<img scr="/矩阵.png" width="70%">
由于涉及到巨大的计算量，要是把算式一个一个列出来再计算效率实在是太慢了，再深度学习中一般把输入输出和参数都放在矩阵之中，在运算的时候能够并行处理，大大提高效率。在深度学习中，可以利用numpy，pytorch，pandas等计算库来执行矩阵计算

* **数据表示**：
一列表示一个feature，一行表示一个sample；参数（权重和偏置）也写在矩阵中；一个图像甚至也可以表示为一个矩阵。能够方便数据表达和运算
* **线性变换**：
在神经网络的层里面，利用矩阵乘法，实现权重和输入的相乘再加上偏置。也就是说，神经网络之中的大部分运算都是矩阵的运算，**优点**就是可以**并行处理**多个样本，提高效率
* **卷积**：
CNN之中，卷积看成是特殊的矩阵运算，比如对应元素相乘，不同channel的对应元素求和，可以用于提取特征

### 常见的激活函数
激活函数在神经网络中很关键，在层与层之间，激活函数引入了**非线性因素**，使得神经网络能够学习和模拟更加复杂的函数映射，否则，例如多个线性回归模型的的叠加最终和一个线性回归模型没有区别，这导致梯度消失。以下是一些最常用的激活函数：

* **Sigmoid 函数**
   - 公式：`σ(x) = 1 / (1 + e^(-x))`
   - 特点：输出范围在(0, 1)之间
   - 应用：常用于二分类问题的输出层
![图片](/.sigmoid.jpg)

* **ReLU 函数（线性整流）**
   - 公式：`f(x) = x if x > 0 else 0`
   - 特点：在x大于0时，函数值为x；否则为0
   - 应用：由于其计算简单和训练效率高，能减少梯度消失的问题
![图片](/relu.webp)

*  **Leaky ReLU**
   - 公式：`f(x) = x if x > 0 else αx`
   - 特点：是ReLU的变体，允许负值有一个非零的梯度
   - 应用：用于解决ReLU的**死亡ReLU问题**(当输入为负时,部分神经元不再更新)。

* **Softmax 函数**
   - 公式：`σ(z) = e^(z) / Σ(e^(z))`
   - 特点：将一个向量映射到另一个向量，使得每个分量都是非负的，并且所有**分量的和为1**。
   - 应用：常用于多分类问题的输出层。
![图片](/softmax.png)


### 机器学习中的数据处理
当然，不只有构造模型，设计优化参数，在实际问题之中，数据处理也是一个问题，收集到的数据如果不加以处理，可能导致错误，因此数据处理是构建有效模型的关键步骤，有助于提高模型的准确率和减少损失值。以下是一些常见的数据处理技术：
* **数据清洗**
   也就是task1中的一部分任务：缺失值、异常值处理，可以利用均值，中位数等填充

* **数据增强**
通过生成新的数据点来扩大训练集
常用于图像，例如通过旋转创建新的图像

* **特征选择**
选择最相关的特征，去除噪声和不相关的特征,例如maxpool也能够去除噪声

* **特征提取**
例如，从文本数据中提取词频，例如利用padding从图像中提取边缘特征

* **数据归一化**
1. 调整数据的尺度，使其在同一范围内，如[-1,1],[0,1]。
归一化通常将数据缩放到[0, 1]范围内，而标准化则将数据转换为均值为0，标准差为1，在[-1,1]的分布
2. 优势：**加速模型收敛**：归一化后的数据通常具有更小的方差，这有助于梯度下降等优化算法更快地收敛到最小损失值。**特征可比对性**：归一化后的特征有统一的尺度，这有助于比较不同特征的重要性，避免了因为单位等不同造成的数据大小的巨大差异

* **数据划分**
将数据集划分为**训练集、验证集和测试集**，划分的比例又是能影响模型训练的效果。、
训练集用于训练模型，验证集用于模型选择和超参数调整，测试集用于最终评估

* **批量处理**
将数据分批处理，设置合适的batch_size,以提高内存效率和计算速度,
在深度学习中，通常使用小批量梯度下降，较小的批量有助于提高模型的泛化能力，但是梯度太小会导致训练不稳定，无法收敛

### 神经网络的基本结构
神经网络是由多个层组成的**计算图**，每层由多个神经元组成。
下面是神经网络的基本结构：
<img scr="/神经网络.png" width="60%">

* **输入层**：
接收原始数据输入，例如各种图像和数据。

* **隐藏层**：
介于输入层和输出层之间，主要用来学习对应关系和提取特征；隐藏层可以不止一个，隐藏层中有多个神经元，每个神经元都与前一层的所有神经元相连。

* **输出层**：
是最后一层，产生模型的最终预测。输出层可以有多个神经元，却决于是分类还是回归等

* **权重和偏置**
![图片](/偏置和权重.webp)
每个连接都有一个权重，和输入相乘加上偏置就是输出
每个神经元都有一个偏置项，使训练不是完全线性的

* **激活函数**
应用于每个神经元的输出，引入非线性因素

* **损失函数**
衡量模型预测与真实标签之间的差异，常见的损失函数有MSE,CrossEntropyLoss等
训练过程中的目标是损失函数值最小，通常意味着良好的模型

* **优化器**
用于优化网络的权重和偏置，得到损失值之后，通过梯度计算，反向传播优化权重，偏置等参数，以减少损失函数值，也就是优化参数
常见的优化器有SGD、Adam等

* **神经网络的训练过程**：
涉及到**前向传播和反向传播**两个阶段。在前向传播中，数据通过计算图向前传播，每层计算输出。在反向传播中，梯度从输出层传播到输入层，用于优化更新权重和偏置


