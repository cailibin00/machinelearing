# Task：vit论文阅读
[github](git@github.com:cailibin00/machinelearning.git)
to be honest,由于全是英文的论文甚至认识单词都需要花费一星期，我便上网查找b站和知乎等网站的讲解，最重要的是和舍友激烈的争论交流之后，利用所学内容对题目的回答便记录在下方，由于时间紧迫，大概率学习到的知识是残缺的，使得解答有错误和残缺。我将会在做题马拉松结束后继续学习。

* **ViT的架构**：
![图片](/模型.jpg)
1. 图像分块：ViT将图像**分割**成几个相同大小相同的小块patches，patches之后可以用来展品放入transformer

2. 线性投影：每个patch通过一个线性层（通常是卷积层）映射到一个高维空间，理解为将图像展品，形成token的嵌入向量。这一步骤将图像数据转换成Transformer模型能够处理的形式。

3. 位置编码：作用是为了给transform提供图像的位置信息，具体有绝对编码、相对编码，实际测试表明相对编码和实际编码表现接近。
位置编码通常使用sin和cos函数实现，将位置信息编码成一个固定长度的多维向量，并且**与token的嵌入向量拼接**，使得嵌入向量包含**位置信息**
![图片](/位置编码.png)

4. transformer编译器：结合位置编码，然后使用**自注意力机制**来捕捉图像中的**特征**，自注意力通常通过**多头注意力**实现（见下文解释6.）。每个编码层的输入和输出还会进行**残差链接**和**层归一化**。

5. MLP Head:通常由一个或两个全连接层组成。在预训练阶段，MLP Head可能由一个**全连接层**后跟一个**非线性激活函数**（如tanh）和一个**输出全连接层**组成,但在微调阶段，通常只需要一个全连接层，这个层将Transformer编码器的输出映射到**类别标签**上！！

6. 自注意力：
<img src="/点积.jpg" width=50%>
<img src="/公式.jpg" width=50%>
**一**：模型每个元素计算一个注意力分数，反映了模型在生成输出时对每个元素的相关程度，由 **查询向量（Query）** 和 **键向量（Key）** 之间的点积决定，为了数据的稳定性，通常除以键-查询空间维度平方根。
**二**：softmax函数，将注意力分数转换为概率分布，0-1，且相加为1（总公式如上图二），得到在0-1的一组数据
**三**：调整各个权重，可以让模型同时预测，也就是每个token之后，预测下一个可能的token，可以让模型得到充分训练机会，在这过程中还会使用**掩码**，减少后方token对前方token影响。（下面图二是用单注意例子解释调整模型参数的过程）
<img src="/参数.jpg" width=50%>
<img src="/单注意.jpg" width=50%>
**四**：将值矩阵分成两个小矩阵相乘，减少参数数量
**五**：后方token的归一化的注意力分数和值矩阵得到的小矩阵**相乘**，得到的向量加到前方的token，得到预测。若是多注意，不止一个注意力头，而是多个单注意**并行**，再将所有的映射得到的向量和后方的token**相加**，得到预测值
六是feed-forward，使用线性回归层fc，增强**泛化能力**，使得模型预测的效果更好。

* **ViT与CNN的比较**：
1. 输入数据：ViT可以处理任意大小的图像，而CNN通常需要固定大小。因为vit把图像分块之后，可以独立处理patch，然后通过位置编码理解位置关系。而CNN通过卷积操作，模型有固定的长款限制。

2. 特征提取：上文所讲，vit可以依赖自注意力来处理，能够注重全局特征和局部特征，而CNN主要提取局部特征。但是vit也有局限，这里摘抄kimi解释的一句话 “在小数据集上，ViT的性能通常不如CNN，因为它需要大量的数据来学习有效的局部特征表示”。

3. ViT需要更多的计算资源和内存，因为Transformer的自注意力机制计算复杂度较高


* **ViT的关键技术创新点**：
1. 将Transformer架构引入到**视觉领域**，利用**自注意力机制**来处理图像数据。
2. 通过Transformer编码器处理图像块的序列，捕捉全局和局部特征。
3. 通过MLPHead进行分类或回归任务。

